import urllib3
from bs4 import BeautifulSoup
import json
import time
import random

http = urllib3.PoolManager(headers={"User-Agent": "Mozilla/5.0"})

MAX_PRODUCTS_PER_CATEGORY = 15  

def get_product_urls(category_url):
    print(f"üì° –°–∫–∞–Ω–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {category_url}")

    try:
        response = http.request('GET', category_url)
        if response.status != 200:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ({response.status}): {category_url}")
            return []
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {category_url}\n{e}")
        return []

    soup = BeautifulSoup(response.data, 'html.parser')
    product_links = []

    for a_tag in soup.find_all("a", class_="catalog_item"):
        link = a_tag.get("href")
        if link and link.startswith("/shop"):
            full_link = "https://qazaqrepublic.com" + link
            product_links.append(full_link)

        if len(product_links) >= MAX_PRODUCTS_PER_CATEGORY:  
            break

    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(product_links)} —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–º–∞–∫—Å–∏–º—É–º {MAX_PRODUCTS_PER_CATEGORY}).")
    return product_links

def get_product_info(url, category_name):
    print(f"üîç –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä: {url}")

    try:
        response = http.request('GET', url)
        if response.status != 200:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–≤–∞—Ä–∞ ({response.status}): {url}")
            return None
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Ç–æ–≤–∞—Ä–∞: {url}\n{e}")
        return None

    soup = BeautifulSoup(response.data, 'html.parser')

    def extract_text(selector, default="–ù–µ –Ω–∞–π–¥–µ–Ω–æ"):
        tag = soup.select_one(selector)
        return tag.get_text(strip=True) if tag else default

    name = extract_text("div.item__name", "–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    price_tag = soup.select_one("div.item__price")
    price = "–¶–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
    if price_tag:
        old_price = price_tag.find("span")  
        if old_price:
            old_price.extract()  
        price = price_tag.get_text(strip=True)

    stock_status = extract_text("div.catalog_item__label", "–í –Ω–∞–ª–∏—á–∏–∏")

    image_url = "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
    image_tag = soup.select_one("div.catalog_item__image img")
    if image_tag and image_tag.get("src"):
        image_url = "https://qazaqrepublic.com" + image_tag["src"]

    product_id = url.rstrip("/").split("/")[-1].split("?")[0]

    return {
        "id": product_id,
        "name": name,
        "price": price,
        "stock_status": stock_status,
        "image_url": image_url,
        "url": url,
        "category": category_name  
    }

category_urls = [
    "https://qazaqrepublic.com/ru/shop/sweatshirts",
    "https://qazaqrepublic.com/ru/shop/hoodies",
    "https://qazaqrepublic.com/ru/shop/top",
    "https://qazaqrepublic.com/ru/shop/t-shirts",
    "https://qazaqrepublic.com/ru/shop/bottom",
    "https://qazaqrepublic.com/ru/shop/headwear",
    "https://qazaqrepublic.com/ru/shop/bags",
    "https://qazaqrepublic.com/ru/shop/kids",
    "https://qazaqrepublic.com/ru/shop/accessories"
]

all_products_by_category = {}

for category_url in category_urls:
    category_name = category_url.split("/")[-1]
    print(f"\nüõí –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {category_name}")

    product_urls = get_product_urls(category_url)
    if not product_urls:
        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é '{category_name}', —Ç–∞–∫ –∫–∞–∫ —Ç–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        continue

    all_products_by_category[category_name] = []

    for count, url in enumerate(product_urls):
        if count >= MAX_PRODUCTS_PER_CATEGORY:
            break  

        product_info = get_product_info(url, category_name)
        if product_info:
            all_products_by_category[category_name].append(product_info)
            print(f"üì¶ {count + 1}/{MAX_PRODUCTS_PER_CATEGORY}: {product_info['name']}")
        else:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ç–æ–≤–∞—Ä–∞: {url}")

        time.sleep(random.uniform(2, 5))  

with open('products_by_category.json', 'w', encoding='utf-8') as file:
    json.dump(all_products_by_category, file, ensure_ascii=False, indent=4)

print("\nüéâ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'products_by_category.json'!")
